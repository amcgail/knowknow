{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(_dh[0].split(\"knowknow\")[0])\n",
    "from knowknow import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "\n",
    "First, you need to get some data. In accordance with JSTOR's usage policies, I **do not provide any full-text data**. And that's the data you need to use this notebook.\n",
    "You can obtain your own data by requesting full OCR data packages through JSTOR's [Data for Research](https://www.jstor.org/dfr/) initiative. \n",
    "\n",
    "Make sure to read carefully through \"User Settings,\" set the appropriate settings, and run the entire notebook.\n",
    "\n",
    "This will create a new \"database\" of counts, which can be recalled by running `my_counts = get_cnt( '<DB_NAME_HERE>' )`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Settings\n",
    "\n",
    "`database_name` is the name you choose for the final dataset of counts\n",
    "\n",
    "`zipdir` is the directory which contains the `.zip` files JSTOR provides to you (not included)\n",
    "\n",
    "`mode` choose between \"basic\" and \"all\" mode\n",
    "\n",
    "1. \"basic\" mode\n",
    "    + this mode is not typically faster than `everything`, but it does reduce RAM overhead\n",
    "        + on ~200k articles the running counters take up more than 16GB RAM\n",
    "        + to counter this, I first run simple statistics, then rerun this notebook again, filtering based on the descriptive statistics\n",
    "    + includes `c` counts, the number of citations each document receives\n",
    "    + includes `c.fj` counts, the number of citations each document receives from each journal's articles\n",
    "    + includes `c.fy` counts, the number of citations each document receives from each year's articles\n",
    "    + includes `fj` counts, the number of citations from each journal\n",
    "    + includes `fj.fy` counts, the number of citations in each journal in each year\n",
    "\n",
    "2. \"all\" mode\n",
    "    + you must run this if you want to run all analyses included in this project\n",
    "    + includes all counts from `basic` mode\n",
    "    + includes \"term\" counts, the counts of tuples appearing in citation contexts (`fj.t` `t` `t.c` `t.fy` `c.t` `t.fy`)\n",
    "    + includes \"author\" counts, the number of citations by each author (`a` `a.c` `a.j.y`)\n",
    "    + includes `c.c`, the cooccurrence network between citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = 'sociology-jstor'\n",
    "zipdir = 'G:/My Drive/projects/qualitative analysis of literature/pre 5-12-2020/003 process JSTOR output/RaW dAtA/'\n",
    "mode = 'all'\n",
    "\n",
    "use_included_citations_filter = True\n",
    "use_included_journals_filter = True\n",
    "\n",
    "NUM_TERMS_TO_KEEP = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not necessary if you're not filtering based on citations and journals pre-count\n",
    "included_citations = load_variable(\"%s.included_citations\"%database_name)\n",
    "included_journals = ['Acta Sociologica', 'Administrative Science Quarterly', 'American Journal of Political Science', 'American Journal of Sociology', 'American Sociological Review', 'Annual Review of Sociology', 'BMS: Bulletin of Sociological Methodology / Bulletin de Méthodologie Sociologique', 'Berkeley Journal of Sociology', 'Contemporary Sociology', 'European Sociological Review', 'Hitotsubashi Journal of Social Studies', 'Humboldt Journal of Social Relations', 'International Journal of Sociology', 'International Journal of Sociology of the Family', 'International Review of Modern Sociology', 'Journal for the Scientific Study of Religion', 'Journal of Health and Social Behavior', 'Journal of Marriage and Family', 'Language in Society', 'Michigan Sociological Review', 'Polish Sociological Review', 'Review of Religious Research', 'Social Forces', 'Social Indicators Research', 'Social Problems', 'Social Psychology Quarterly', 'Sociological Bulletin', 'Sociological Focus', 'Sociological Forum', 'Sociological Methodology', 'Sociological Perspectives', 'Sociological Theory', 'Sociology', 'Sociology of Education', 'Sociology of Religion', 'Symbolic Interaction', 'The American Sociologist', 'The British Journal of Sociology', 'The Canadian Journal of Sociology', 'The Sociological Quarterly', 'Theory and Society']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_map = {} # default\n",
    "journal_map = {\n",
    "    \"Canadian Journal of Sociology / Cahiers canadiens de sociologie\": 'The Canadian Journal of Sociology',\n",
    "    \"The Canadian Journal of Sociology / Cahiers canadiens de\\n                sociologie\": 'The Canadian Journal of Sociology',\n",
    "    'The Canadian Journal of Sociology / Cahiers canadiens de sociologie': 'The Canadian Journal of Sociology'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "from nltk import sent_tokenize\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# library functions for cleaning and extracting in-text citations from OCR\n",
    "from cnt_cooc_jstor_lib import (\n",
    "    extractCitation, getOuterParens, \n",
    "    Document, ParseError, \n",
    "    clean_metadata\n",
    ")\n",
    "\n",
    "# XML parser\n",
    "from lxml.etree import _ElementTree as ElementTree\n",
    "from lxml import etree\n",
    "recovering_parser = etree.XMLParser(recover=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following helper function `file_iterator` iterates through all documents inside a list of zipfiles\n",
    "\n",
    "Each iteration returns:\n",
    "1. the document DOI\n",
    "2. the metadata file contents\n",
    "3. the ocr file contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getname(x):\n",
    "    x = x.split(\"/\")[-1]\n",
    "    x = re.sub(r'(\\.xml|\\.txt)','',x)\n",
    "    return x\n",
    "\n",
    "def file_iterator(zipfiles):\n",
    "    from random import shuffle\n",
    "    \n",
    "    all_files = []\n",
    "    for zf in zipfiles:\n",
    "        archive = ZipFile(zf, 'r')\n",
    "        files = archive.namelist()\n",
    "        names = list(set(getname(x) for x in files))\n",
    "        \n",
    "        all_files += [(archive,name) for name in names]\n",
    "        \n",
    "    shuffle(all_files)\n",
    "        \n",
    "    for archive, name in all_files:\n",
    "        try:\n",
    "            yield(\n",
    "                name.split(\"-\")[-1].replace(\"_\", \"/\"),\n",
    "                archive.read(\"metadata/%s.xml\" % name),\n",
    "                archive.read(\"ocr/%s.txt\" % name).decode('utf8')\n",
    "            )\n",
    "        except KeyError: # some very few articles don't have both\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_page_strings` takes the string contents of an XML file produced by JSTOR. The XML file in question represents the text of a given article. This function cleans it, and splits it into pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_ocr_cleaning(x):\n",
    "    # remove multiple spaces in a row\n",
    "    x = re.sub(r\" +\", ' ', str(x))\n",
    "    # remove hyphenations [NOTE this should be updated, with respect to header and footer across pages...]\n",
    "    x = re.sub(r\"([A-Za-z]+)-\\s+([A-Za-z]+)\", \"\\g<1>\\g<2>\", x)\n",
    "    \n",
    "    x = x.strip()\n",
    "    return x\n",
    "\n",
    "def get_content_string(ocr_string):\n",
    "    docXml = etree.fromstring(ocr_string, parser=recovering_parser)\n",
    "    pages = docXml.findall(\".//page\")\n",
    "\n",
    "    page_strings = []\n",
    "    for p in pages:\n",
    "        if p.text is None:\n",
    "            continue\n",
    "        page_strings.append(p.text)\n",
    "\n",
    "    secs = docXml.findall(\".//sec\")\n",
    "\n",
    "    for s in secs:\n",
    "        if s.text is None:\n",
    "            continue\n",
    "        if s.text.strip() == '':\n",
    "            try_another = etree.tostring(s, encoding='utf8', method='text').decode(\"utf8\").strip()\n",
    "            #print(try_another)\n",
    "            if try_another == '':\n",
    "                continue\n",
    "\n",
    "            page_strings.append(try_another)\n",
    "        else:\n",
    "            page_strings.append(s.text.strip())\n",
    "\n",
    "    return basic_ocr_cleaning( \"\\n\\n\".join(page_strings) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_blacklist = set()\n",
    "\n",
    "def consolidate_terms():\n",
    "    global term_blacklist\n",
    "    \n",
    "\n",
    "    # this is where the filtering occurs\n",
    "    \n",
    "    if False:\n",
    "        # takes terms based on a minimum count, a cutoff\n",
    "        cutoff = 10\n",
    "        \n",
    "        terms = list(cnt_doc['t'].keys())\n",
    "        counts = np.array([cnt_doc['t'][k] for k in terms])\n",
    "        \n",
    "        to_remove = set([terms[int(i)] for i in np.argwhere(counts < cutoff)])\n",
    "    \n",
    "    if True:\n",
    "        # takes the top 5000 terms in terms of yearly count\n",
    "        sort_them = sorted(cnt_doc['fy.t'], key=lambda x: -cnt_doc['fy.t'][x])\n",
    "        have_now = set(cnt_doc['t'])\n",
    "        to_keep = set()\n",
    "        \n",
    "        i = 0\n",
    "        while len(to_keep) < NUM_TERMS_TO_KEEP and i < len(sort_them):\n",
    "            to_keep.add(sort_them[i][1]) # adds the term, not the year-term pair, to the set\n",
    "            i += 1\n",
    "        to_remove = have_now.difference(to_keep)\n",
    "    \n",
    "    # so that we never log counts for these again:\n",
    "    term_blacklist.update(to_remove)\n",
    "\n",
    "    # the rest of the code is pruning all other term counts for this term in memory\n",
    "    print(\"consolidating\", len(to_remove), list(to_remove)[:5])\n",
    "    \n",
    "    to_prune = ['t','fy.t','fj.t','c.t']\n",
    "    for tp in to_prune:\n",
    "        \n",
    "        whichT = tp.split(\".\").index('t') # this checks where 't' is in the name of the variable (first or second?)\n",
    "\n",
    "        print(\"pruning '%s'...\" % tp)\n",
    "\n",
    "        if tp == 't':\n",
    "            tydels = to_remove\n",
    "        else:\n",
    "            tydels = [x for x in cnt_doc[tp] if x[ whichT ] in to_remove]\n",
    "            \n",
    "        print(\"old size:\", len(cnt_doc[tp]))\n",
    "        for tr in tydels:\n",
    "            del cnt_doc[tp][tr]\n",
    "            del cnt_ind[tp][tr]\n",
    "        print(\"new size:\", len(cnt_doc[tp]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cits = 0\n",
    "last_print = 0\n",
    "\n",
    "def account_for(doc):\n",
    "    global cits, last_print, mode\n",
    "    \n",
    "    # consolidating \"terms\" counter as I go, to limit RAM overhead\n",
    "    # I'm only interested in the most common 1000\n",
    "    if cits - last_print > 100000:\n",
    "        print(\"Citation %s\" % cits)\n",
    "        print(\"Term %s\" % len(cnt_doc['t']))\n",
    "        #print(sample(list(cnt_doc['t']), 10))\n",
    "        last_print = cits\n",
    "        consolidate_terms()\n",
    "\n",
    "\n",
    "    if 'citations' not in doc or not len(doc['citations']):\n",
    "        #print(\"No citations\", doc['doi'])\n",
    "        return\n",
    "\n",
    "    for c in doc['citations']:\n",
    "        if 'contextPure' not in c:\n",
    "            raise Exception(\"no contextPure...\")\n",
    "\n",
    "        sp = c['contextPure'].lower()\n",
    "        sp = re.sub(\"[,-`\\\"'.“]\", \"\", sp) # removing extraneous characters\n",
    "        sp = re.sub(\"\\s+\", \" \", sp) # removing extra characters\n",
    "        sp = sp.split() # splitting into words\n",
    "\n",
    "\n",
    "        for cited in c['citations']:\n",
    "            cits += 1\n",
    "            cnt(doc['year'], 'fy', doc['doi'])\n",
    "\n",
    "            # citation\n",
    "            cnt(cited, 'c', doc['doi'])\n",
    "\n",
    "            # journal\n",
    "            cnt(doc['journal'], 'fj', doc['doi'])\n",
    "\n",
    "            # journal year\n",
    "            cnt((doc['journal'], doc['year']), 'fj.fy', doc['doi'])\n",
    "\n",
    "            # citation journal\n",
    "            cnt((cited, doc['journal']), 'c.fj', doc['doi'])\n",
    "\n",
    "            # citation year\n",
    "            cnt((cited, doc['year']), 'c.fy', doc['doi'])\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        if mode == 'all':\n",
    "\n",
    "            sp = [x for x in sp if x not in stopwords] # strip stopwords\n",
    "            tups = set(zip(sp[:-1], sp[1:])) # two-word tuples\n",
    "            tups = [x for x in tups if x not in term_blacklist]\n",
    "            \n",
    "\n",
    "            for cited in c['citations']:\n",
    "                \n",
    "                if use_included_citations_filter and (cited not in included_citations):\n",
    "                    continue\n",
    "                \n",
    "                # term features\n",
    "                for t1 in tups:\n",
    "                    \n",
    "                    # cited work, tuple\n",
    "                    cnt((cited, t1), 'c.t', doc['doi'])\n",
    "\n",
    "                    # term\n",
    "                    cnt(t1, 't', doc['doi'])\n",
    "\n",
    "                    # term year\n",
    "                    cnt((doc['year'], t1), 'fy.t', doc['doi'])\n",
    "\n",
    "                    # term journal\n",
    "                    cnt((doc['journal'], t1), 'fj.t', doc['doi'])\n",
    "\n",
    "                    if False: # eliminating data I'm not using\n",
    "\n",
    "                        # author loop\n",
    "                        for a in doc['authors']:\n",
    "                            # term author\n",
    "                            cnt((a, t1), 'a.t', doc['doi'])\n",
    "\n",
    "                        # term term...\n",
    "                        for t2 in present_terms:\n",
    "                            if t1 >= t2:\n",
    "                                continue\n",
    "\n",
    "                            if t1 in t2 or t2 in t1:\n",
    "                                continue\n",
    "\n",
    "                            # term term\n",
    "                            cnt((t1,t2), 't.t', doc['doi'])\n",
    "\n",
    "                # author loop\n",
    "                for a in doc['authors']:\n",
    "                    # citation author\n",
    "                    cnt((a, cited), 'a.c', doc['doi'])\n",
    "\n",
    "                    # year author journal\n",
    "                    cnt((a, doc['journal'], doc['year']), 'a.fj.fy', doc['doi'])\n",
    "\n",
    "                    # author\n",
    "                    cnt(a, 'a', doc['doi'])\n",
    "\n",
    "                # add to counters for citation-citation counts\n",
    "                for cited1 in c['citations']:\n",
    "                    for cited2 in c['citations']:\n",
    "                        if cited1 >= cited2:\n",
    "                            continue\n",
    "\n",
    "                        cnt(( cited1, cited2 ), 'c.c', doc['doi'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "faulty_years = set()\n",
    "\n",
    "def citation_iterator(citations):\n",
    "    for c in citations:\n",
    "        name = c['names']\n",
    "        name = name.replace(\"'s\",\"\") # change \"Simmel's\" to \"Simmel\"\n",
    "        \n",
    "        years = c['year'].split(\",\")\n",
    "        for y in years:\n",
    "            y = y.strip() # get rid of the extra spacing\n",
    "            y = re.findall(r'[0-9]{4}',y) # this strips out the 'a' in '1995a'\n",
    "            if not len(y):\n",
    "                faulty_years.add(c['year'])\n",
    "                continue\n",
    "            \n",
    "            y = y[0]\n",
    "            yield \"%s (%s)\" % (name, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_ind = defaultdict(lambda:defaultdict(int))\n",
    "track_doc = defaultdict(lambda:defaultdict(set))\n",
    "cnt_doc = defaultdict(lambda:defaultdict(int))\n",
    "\n",
    "def cnt(term, space, doc):\n",
    "    # it's a set, yo\n",
    "    track_doc[space][term].add(doc)\n",
    "    # update cnt_doc\n",
    "    cnt_doc[space][term] = len(track_doc[space][term])\n",
    "    # update ind count\n",
    "    cnt_ind[space][term] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting ready for term counting\n",
    "from nltk.corpus import stopwords as sw\n",
    "stopwords = set(sw.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipfiles = list(Path(zipdir).glob(\"*.zip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 ... 0 journals so far\n",
      "Document 1000 ... 38 journals so far\n",
      "Document 2000 ... 40 journals so far\n",
      "Document 3000 ... 40 journals so far\n",
      "Document 4000 ... 40 journals so far\n",
      "Document 5000 ... 41 journals so far\n",
      "Document 6000 ... 41 journals so far\n",
      "Document 7000 ... 41 journals so far\n",
      "Document 8000 ... 41 journals so far\n",
      "Document 9000 ... 41 journals so far\n",
      "Document 10000 ... 41 journals so far\n",
      "Citation 100034\n",
      "Term 435089\n",
      "consolidating 434089 [('samesex', 'relationships'), ('emphasize', 'sociological'), ('shows', 'contact'), ('specifically', 'claim'), ('hand', 'refers')]\n",
      "pruning 't'...\n",
      "old size: 435089\n",
      "new size: 1000\n",
      "pruning 'fy.t'...\n",
      "old size: 533619\n",
      "new size: 13451\n",
      "pruning 'fj.t'...\n",
      "old size: 520810\n",
      "new size: 10644\n",
      "pruning 'c.t'...\n",
      "old size: 943464\n",
      "new size: 47868\n",
      "Document 11000 ... 41 journals so far\n",
      "Document 12000 ... 41 journals so far\n",
      "Document 13000 ... 41 journals so far\n",
      "Document 14000 ... 41 journals so far\n",
      "Document 15000 ... 41 journals so far\n",
      "Document 16000 ... 41 journals so far\n",
      "Document 17000 ... 41 journals so far\n",
      "Document 18000 ... 41 journals so far\n",
      "Document 19000 ... 41 journals so far\n",
      "Document 20000 ... 41 journals so far\n",
      "Document 21000 ... 41 journals so far\n",
      "Citation 200071\n",
      "Term 430429\n",
      "consolidating 429429 [('samesex', 'relationships'), ('(devaluation', 'ordinary'), ('copies', 'high‐risk'), ('mayjune', 'france'), ('breakage', 'times')]\n",
      "pruning 't'...\n",
      "old size: 430429\n",
      "new size: 1000\n",
      "pruning 'fy.t'...\n",
      "old size: 533816\n",
      "new size: 19715\n",
      "pruning 'fj.t'...\n",
      "old size: 519659\n",
      "new size: 14567\n",
      "pruning 'c.t'...\n",
      "old size: 971844\n",
      "new size: 89511\n",
      "Document 22000 ... 41 journals so far\n",
      "Document 23000 ... 41 journals so far\n",
      "Document 24000 ... 41 journals so far\n",
      "Document 25000 ... 41 journals so far\n",
      "Document 26000 ... 41 journals so far\n",
      "Document 27000 ... 41 journals so far\n",
      "Document 28000 ... 41 journals so far\n",
      "Document 29000 ... 41 journals so far\n",
      "Document 30000 ... 41 journals so far\n",
      "Document 31000 ... 41 journals so far\n",
      "Citation 300078\n",
      "Term 433122\n",
      "consolidating 432122 [('samesex', 'relationships'), ('information', '(webster'), ('constitute', 'common'), ('scientific', 'philosophical'), ('abandon', 'inappropriate')]\n",
      "pruning 't'...\n",
      "old size: 433122\n",
      "new size: 1000\n",
      "pruning 'fy.t'...\n",
      "old size: 539768\n",
      "new size: 24225\n",
      "pruning 'fj.t'...\n",
      "old size: 523470\n",
      "new size: 17202\n",
      "pruning 'c.t'...\n",
      "old size: 1008021\n",
      "new size: 130304\n",
      "Document 32000 ... 41 journals so far\n",
      "Document 33000 ... 41 journals so far\n",
      "Document 34000 ... 41 journals so far\n",
      "Document 35000 ... 41 journals so far\n",
      "Document 36000 ... 41 journals so far\n",
      "Document 37000 ... 41 journals so far\n",
      "Document 38000 ... 41 journals so far\n",
      "Document 39000 ... 41 journals so far\n",
      "Document 40000 ... 41 journals so far\n",
      "Document 41000 ... 41 journals so far\n",
      "Document 42000 ... 41 journals so far\n",
      "Citation 400218\n",
      "Term 425413\n",
      "consolidating 424413 [('establishing', 'mal'), ('samesex', 'relationships'), ('meaningful', 'communityspecific'), ('trust', 'attracted'), ('constitute', 'common')]\n",
      "pruning 't'...\n",
      "old size: 425413\n",
      "new size: 1000\n",
      "pruning 'fy.t'...\n",
      "old size: 533256\n",
      "new size: 26991\n",
      "pruning 'fj.t'...\n",
      "old size: 515257\n",
      "new size: 18889\n",
      "pruning 'c.t'...\n",
      "old size: 1048389\n",
      "new size: 170711\n",
      "Document 43000 ... 41 journals so far\n",
      "Document 44000 ... 41 journals so far\n",
      "Document 45000 ... 41 journals so far\n",
      "Document 46000 ... 41 journals so far\n",
      "Document 47000 ... 41 journals so far\n",
      "Document 48000 ... 41 journals so far\n",
      "Document 49000 ... 41 journals so far\n",
      "Document 50000 ... 41 journals so far\n",
      "Document 51000 ... 41 journals so far\n",
      "Document 52000 ... 41 journals so far\n",
      "Citation 500226\n",
      "Term 422872\n",
      "consolidating 421872 [('firm', 'indicator'), ('risk', 'offering'), ('integration', 'reformulations'), ('tend', 'monsterbarring'), ('awkward', 'problem')]\n",
      "pruning 't'...\n",
      "old size: 422872\n",
      "new size: 1000\n",
      "pruning 'fy.t'...\n",
      "old size: 532961\n",
      "new size: 28483\n",
      "pruning 'fj.t'...\n",
      "old size: 514225\n",
      "new size: 19792\n",
      "pruning 'c.t'...\n",
      "old size: 1085371\n",
      "new size: 206999\n",
      "Document 53000 ... 41 journals so far\n",
      "Document 54000 ... 41 journals so far\n",
      "Document 55000 ... 41 journals so far\n",
      "Document 56000 ... 41 journals so far\n",
      "Document 57000 ... 41 journals so far\n",
      "Document 58000 ... 41 journals so far\n",
      "Document 59000 ... 41 journals so far\n",
      "Document 60000 ... 41 journals so far\n",
      "Document 61000 ... 41 journals so far\n",
      "Document 62000 ... 41 journals so far\n",
      "Citation 600243\n",
      "Term 428290\n",
      "consolidating 427290 [('constitute', 'common'), ('child', 'low'), ('scientific', 'philosophical'), ('mindset', 'soldiers'), ('specifically', 'claim')]\n",
      "pruning 't'...\n",
      "old size: 428290\n",
      "new size: 1000\n",
      "pruning 'fy.t'...\n",
      "old size: 539253\n",
      "new size: 30704\n",
      "pruning 'fj.t'...\n",
      "old size: 520210\n",
      "new size: 21299\n",
      "pruning 'c.t'...\n",
      "old size: 1128629\n",
      "new size: 248161\n",
      "Document 63000 ... 41 journals so far\n",
      "Document 64000 ... 41 journals so far\n",
      "Document 65000 ... 41 journals so far\n",
      "Document 66000 ... 41 journals so far\n",
      "Document 67000 ... 41 journals so far\n",
      "Document 68000 ... 41 journals so far\n",
      "Document 69000 ... 41 journals so far\n",
      "Document 70000 ... 41 journals so far\n",
      "Document 71000 ... 41 journals so far\n",
      "Citation 700297\n",
      "Term 432246\n",
      "consolidating 431246 [('samesex', 'relationships'), ('cooptive', 'arrangement'), ('constitute', 'common'), ('upperclass', 'solidarity'), ('academic', 'becomes')]\n",
      "pruning 't'...\n",
      "old size: 432246\n",
      "new size: 1000\n",
      "pruning 'fy.t'...\n",
      "old size: 546192\n",
      "new size: 32220\n",
      "pruning 'fj.t'...\n",
      "old size: 526980\n",
      "new size: 22362\n",
      "pruning 'c.t'...\n",
      "old size: 1159041\n",
      "new size: 286222\n",
      "Document 72000 ... 41 journals so far\n",
      "Document 73000 ... 41 journals so far\n",
      "Document 74000 ... 41 journals so far\n",
      "Document 75000 ... 41 journals so far\n",
      "Document 76000 ... 41 journals so far\n",
      "Document 77000 ... 41 journals so far\n",
      "Document 78000 ... 41 journals so far\n",
      "Document 79000 ... 41 journals so far\n",
      "Document 80000 ... 41 journals so far\n",
      "Document 81000 ... 41 journals so far\n",
      "Document 82000 ... 41 journals so far\n",
      "Citation 800338\n",
      "Term 430978\n",
      "consolidating 429978 [('samesex', 'relationships'), ('emphasize', 'sociological'), ('conclusions', 'relative'), ('selected', 'often'), ('ranieri', 'p')]\n",
      "pruning 't'...\n",
      "old size: 430978\n",
      "new size: 1000\n",
      "pruning 'fy.t'...\n",
      "old size: 549623\n",
      "new size: 32928\n",
      "pruning 'fj.t'...\n",
      "old size: 529598\n",
      "new size: 22962\n",
      "pruning 'c.t'...\n",
      "old size: 1222180\n",
      "new size: 323034\n",
      "Document 83000 ... 41 journals so far\n",
      "Document 84000 ... 41 journals so far\n",
      "parse error... ('No valid year found',) 10.2307/26650789\n",
      "Document 85000 ... 41 journals so far\n",
      "Document 86000 ... 41 journals so far\n",
      "Document 87000 ... 41 journals so far\n",
      "Document 88000 ... 41 journals so far\n",
      "Document 89000 ... 41 journals so far\n",
      "Document 90000 ... 41 journals so far\n",
      "Document 91000 ... 41 journals so far\n",
      "Document 92000 ... 41 journals so far\n",
      "Citation 900392\n",
      "Term 434107\n",
      "consolidating 433107 [('process', 'desacralization'), ('emphasize', 'sociological'), ('samesex', 'relationships'), ('insurgency', 'brought'), ('reality', 'became')]\n",
      "pruning 't'...\n",
      "old size: 434107\n",
      "new size: 1000\n",
      "pruning 'fy.t'...\n",
      "old size: 551947\n",
      "new size: 34351\n",
      "pruning 'fj.t'...\n",
      "old size: 532488\n",
      "new size: 23832\n",
      "pruning 'c.t'...\n",
      "old size: 1243494\n",
      "new size: 363506\n",
      "Document 93000 ... 41 journals so far\n",
      "Document 94000 ... 41 journals so far\n",
      "Document 95000 ... 41 journals so far\n",
      "Document 96000 ... 41 journals so far\n",
      "Document 97000 ... 41 journals so far\n",
      "Document 98000 ... 41 journals so far\n",
      "Document 99000 ... 41 journals so far\n",
      "Document 100000 ... 41 journals so far\n",
      "Document 101000 ... 41 journals so far\n",
      "Document 102000 ... 41 journals so far\n",
      "Document 103000 ... 41 journals so far\n",
      "Citation 1000424\n",
      "Term 429934\n",
      "consolidating 428934 [('samesex', 'relationships'), ('killings', 'even'), ('however', 'marketbased'), ('law', 'assigned'), ('construction', 'feces')]\n",
      "pruning 't'...\n",
      "old size: 429934\n",
      "new size: 1000\n",
      "pruning 'fy.t'...\n",
      "old size: 546222\n",
      "new size: 35112\n",
      "pruning 'fj.t'...\n",
      "old size: 525536\n",
      "new size: 24365\n",
      "pruning 'c.t'...\n",
      "old size: 1274268\n",
      "new size: 399871\n",
      "Document 104000 ... 41 journals so far\n",
      "Document 105000 ... 41 journals so far\n",
      "Document 106000 ... 41 journals so far\n",
      "Document 107000 ... 41 journals so far\n",
      "Document 108000 ... 41 journals so far\n",
      "Document 109000 ... 41 journals so far\n",
      "Document 110000 ... 41 journals so far\n",
      "Document 111000 ... 41 journals so far\n",
      "Document 112000 ... 41 journals so far\n",
      "Document 113000 ... 41 journals so far\n",
      "Citation 1100463\n",
      "Term 427686\n",
      "consolidating 426686 [('samesex', 'relationships'), ('may', 'lies'), ('audience', 'forces”'), ('deference', 'valuation'), ('shows', 'contact')]\n",
      "pruning 't'...\n",
      "old size: 427686\n",
      "new size: 1000\n",
      "pruning 'fy.t'...\n",
      "old size: 542858\n",
      "new size: 35537\n",
      "pruning 'fj.t'...\n",
      "old size: 523265\n",
      "new size: 24797\n",
      "pruning 'c.t'...\n",
      "old size: 1307965\n",
      "new size: 434200\n",
      "Document 114000 ... 41 journals so far\n",
      "Document 115000 ... 41 journals so far\n",
      "Document 116000 ... 41 journals so far\n",
      "Document 117000 ... 41 journals so far\n",
      "Document 118000 ... 41 journals so far\n",
      "Document 119000 ... 41 journals so far\n",
      "Document 120000 ... 41 journals so far\n",
      "Document 121000 ... 41 journals so far\n",
      "Document 122000 ... 41 journals so far\n",
      "Document 123000 ... 41 journals so far\n",
      "Citation 1200487\n",
      "Term 427014\n",
      "consolidating 426014 [('samesex', 'relationships'), ('instances', 'emerged'), ('conclusions', 'relative'), (')', 'interpretations'), ('hand', 'refers')]\n",
      "pruning 't'...\n",
      "old size: 427014\n",
      "new size: 1000\n",
      "pruning 'fy.t'...\n",
      "old size: 544537\n",
      "new size: 36613\n",
      "pruning 'fj.t'...\n",
      "old size: 524872\n",
      "new size: 25618\n",
      "pruning 'c.t'...\n",
      "old size: 1342497\n",
      "new size: 471610\n",
      "Document 124000 ... 41 journals so far\n",
      "Document 125000 ... 41 journals so far\n",
      "Document 126000 ... 41 journals so far\n",
      "Document 127000 ... 41 journals so far\n",
      "Document 128000 ... 41 journals so far\n",
      "Document 129000 ... 41 journals so far\n",
      "Document 130000 ... 41 journals so far\n",
      "Document 131000 ... 41 journals so far\n",
      "Document 132000 ... 41 journals so far\n",
      "Document 133000 ... 41 journals so far\n",
      "parse error... ('No valid year found',) 10.2307/26650770\n",
      "Document 134000 ... 41 journals so far\n",
      "Citation 1300643\n",
      "Term 435378\n",
      "consolidating 434378 [('samesex', 'relationships'), ('profiling', 'occur'), ('uc', 'berkeley'), ('scientific', 'philosophical'), (')', 'interpretations')]\n",
      "pruning 't'...\n",
      "old size: 435378\n",
      "new size: 1000\n",
      "pruning 'fy.t'...\n",
      "old size: 553511\n",
      "new size: 36398\n",
      "pruning 'fj.t'...\n",
      "old size: 533497\n",
      "new size: 25645\n",
      "pruning 'c.t'...\n",
      "old size: 1390544\n",
      "new size: 500836\n",
      "Document 135000 ... 41 journals so far\n",
      "Document 136000 ... 41 journals so far\n",
      "Document 137000 ... 41 journals so far\n",
      "Document 138000 ... 41 journals so far\n",
      "Document 139000 ... 41 journals so far\n",
      "Document 140000 ... 41 journals so far\n",
      "Document 141000 ... 41 journals so far\n",
      "Document 142000 ... 41 journals so far\n",
      "Document 143000 ... 41 journals so far\n",
      "Document 144000 ... 41 journals so far\n",
      "Citation 1400649\n",
      "Term 436408\n",
      "consolidating 435408 [('samesex', 'relationships'), ('(devaluation', 'ordinary'), ('included', 'heterodox'), ('enacted', 'combination'), ('elements', 'movement')]\n",
      "pruning 't'...\n",
      "old size: 436408\n",
      "new size: 1000\n",
      "pruning 'fy.t'...\n",
      "old size: 556693\n",
      "new size: 37427\n",
      "pruning 'fj.t'...\n",
      "old size: 537266\n",
      "new size: 26434\n",
      "pruning 'c.t'...\n",
      "old size: 1421663\n",
      "new size: 539075\n",
      "Document 145000 ... 41 journals so far\n",
      "Document 146000 ... 41 journals so far\n",
      "Document 147000 ... 41 journals so far\n",
      "Document 148000 ... 41 journals so far\n",
      "Document 149000 ... 41 journals so far\n",
      "Document 150000 ... 41 journals so far\n",
      "Document 151000 ... 41 journals so far\n",
      "Document 152000 ... 41 journals so far\n",
      "Document 153000 ... 41 journals so far\n",
      "Document 154000 ... 41 journals so far\n",
      "Citation 1500682\n",
      "Term 424232\n",
      "consolidating 423232 [('samesex', 'relationships'), ('sudden', 'burst'), ('home', 'distant'), ('post', 'index'), ('investment)', 'would')]\n",
      "pruning 't'...\n",
      "old size: 424232\n",
      "new size: 1000\n",
      "pruning 'fy.t'...\n",
      "old size: 540624\n",
      "new size: 37575\n",
      "pruning 'fj.t'...\n",
      "old size: 521369\n",
      "new size: 26699\n",
      "pruning 'c.t'...\n",
      "old size: 1443217\n",
      "new size: 571644\n",
      "Document 155000 ... 41 journals so far\n",
      "Document 156000 ... 41 journals so far\n",
      "Document 157000 ... 41 journals so far\n",
      "Document 158000 ... 41 journals so far\n",
      "Document 159000 ... 41 journals so far\n",
      "Document 160000 ... 41 journals so far\n",
      "Document 161000 ... 41 journals so far\n",
      "Document 162000 ... 41 journals so far\n",
      "Document 163000 ... 41 journals so far\n",
      "Document 164000 ... 41 journals so far\n",
      "Document 165000 ... 41 journals so far\n",
      "Citation 1600684\n",
      "Term 429644\n",
      "consolidating 428644 [('samesex', 'relationships'), ('instances', 'emerged'), ('possession', 'attributes'), ('addition', 'raw'), ('writes', 'margalit')]\n",
      "pruning 't'...\n",
      "old size: 429644\n",
      "new size: 1000\n",
      "pruning 'fy.t'...\n",
      "old size: 549750\n",
      "new size: 38240\n",
      "pruning 'fj.t'...\n",
      "old size: 530055\n",
      "new size: 27288\n",
      "pruning 'c.t'...\n",
      "old size: 1483274\n",
      "new size: 606889\n",
      "Document 166000 ... 41 journals so far\n",
      "Document 167000 ... 41 journals so far\n",
      "Document 168000 ... 41 journals so far\n",
      "Document 169000 ... 41 journals so far\n",
      "Document 170000 ... 41 journals so far\n",
      "Document 171000 ... 41 journals so far\n",
      "Document 172000 ... 41 journals so far\n",
      "Document 173000 ... 41 journals so far\n",
      "Document 174000 ... 41 journals so far\n",
      "Document 175000 ... 41 journals so far\n",
      "Citation 1700700\n",
      "Term 438145\n",
      "consolidating 437145 [('helped', 'provoke'), ('therapeutic', 'health'), ('samesex', 'relationships'), ('addition', 'raw'), ('embraced', 'peripheral')]\n",
      "pruning 't'...\n",
      "old size: 438145\n",
      "new size: 1000\n",
      "pruning 'fy.t'...\n",
      "old size: 557709\n",
      "new size: 38760\n",
      "pruning 'fj.t'...\n",
      "old size: 538488\n",
      "new size: 27708\n",
      "pruning 'c.t'...\n",
      "old size: 1521541\n",
      "new size: 639070\n",
      "Document 176000 ... 41 journals so far\n",
      "Document 177000 ... 41 journals so far\n",
      "Document 178000 ... 41 journals so far\n",
      "Document 179000 ... 41 journals so far\n",
      "Document 180000 ... 41 journals so far\n",
      "Document 181000 ... 41 journals so far\n",
      "Document 182000 ... 41 journals so far\n",
      "Document 183000 ... 41 journals so far\n",
      "Document 184000 ... 41 journals so far\n"
     ]
    }
   ],
   "source": [
    "seen = set()\n",
    "\n",
    "skipped = 0\n",
    "\n",
    "total_count = Counter()\n",
    "doc_count = Counter()\n",
    "pair_count = Counter()\n",
    "\n",
    "debug = False\n",
    "\n",
    "for i, (doi, metadata_str, ocr_str) in enumerate( file_iterator(zipfiles) ):\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(\"Document\", i, \"...\", len(cnt_doc['fj'].keys()), \"journals so far\")\n",
    "\n",
    "    try:\n",
    "        drep = clean_metadata( doi, metadata_str )\n",
    "        \n",
    "        # sometimes multiple journal names map onto the same journal, for all intents and purposes\n",
    "        if drep['journal'] in journal_map:\n",
    "            drep['journal'] = journal_map[drep['journal']]\n",
    "        \n",
    "        # only include journals in the list \"included_journals\"\n",
    "        if use_included_journals_filter and (drep['journal'] not in included_journals):\n",
    "            continue\n",
    "        \n",
    "        if debug: print(\"got meta\")\n",
    "\n",
    "        if drep['type'] != 'research-article':\n",
    "            continue\n",
    "            \n",
    "        # some types of titles should be immediately ignored\n",
    "        def title_looks_researchy(lt):\n",
    "            lt = lt.lower()\n",
    "            lt = lt.strip()\n",
    "\n",
    "            for x in [\"book review\", 'review essay', 'back matter', 'front matter', 'notes for contributors', 'publication received', 'errata:', 'erratum:']:\n",
    "                if x in lt:\n",
    "                    return False\n",
    "\n",
    "            for x in [\"commentary and debate\", 'erratum', '']:\n",
    "                if x == lt:\n",
    "                    return False\n",
    "\n",
    "            return True\n",
    "\n",
    "        lt = drep['title'].lower()\n",
    "        if not title_looks_researchy(lt):\n",
    "            continue\n",
    "\n",
    "        # Don't process the document if there are no authors\n",
    "        if not len(drep['authors']):\n",
    "            continue\n",
    "\n",
    "        drep['content'] = get_content_string(ocr_str)\n",
    "        \n",
    "        drep['citations'] = []\n",
    "        \n",
    "        # loop through the matching parentheses in the document\n",
    "        for index, (parenStart, parenContents) in enumerate(getOuterParens(drep['content'])):\n",
    "            citations = extractCitation(parenContents)\n",
    "            if not len(citations):\n",
    "                continue\n",
    "\n",
    "            citations = list(citation_iterator(citations))\n",
    "                \n",
    "            citation = {\n",
    "                \"citations\": citations,\n",
    "                \"contextLeft\": drep['content'][parenStart-400+1:parenStart+1],\n",
    "                \"contextRight\": drep['content'][parenStart + len(parenContents) + 1:parenStart + len(parenContents) + 1 + 100],\n",
    "                \"where\": parenStart\n",
    "            }\n",
    "\n",
    "\n",
    "            # cut off any stuff before the first space\n",
    "            first_break_left = re.search(r\"[\\s\\.!\\?]+\", citation['contextLeft'])\n",
    "            if first_break_left is not None:\n",
    "                clean_start_left = citation['contextLeft'][first_break_left.end():]\n",
    "            else:\n",
    "                clean_start_left = citation['contextLeft']\n",
    "\n",
    "            # cut off any stuff after the last space\n",
    "            last_break_right = list(re.finditer(r\"[\\s\\.!\\?]+\", citation['contextRight']))\n",
    "            if len(last_break_right):\n",
    "                clean_end_right = citation['contextRight'][:last_break_right[-1].start()]\n",
    "            else:\n",
    "                clean_end_right = citation['contextRight']\n",
    "\n",
    "            # we don't want anything more than a sentence\n",
    "            \n",
    "            sentence_left = sent_tokenize(clean_start_left)\n",
    "            if len(sentence_left):\n",
    "                sentence_left = sentence_left[-1]\n",
    "            else:\n",
    "                sentence_left = \"\"\n",
    "\n",
    "            sentence_right = sent_tokenize(clean_end_right)[0]\n",
    "            if len(sentence_right):\n",
    "                sentence_right = sentence_right[0]\n",
    "            else:\n",
    "                sentence_right = \"\"\n",
    "\n",
    "            # finally, strip the parentheses from the string\n",
    "            sentence_left = sentence_left[:-1]\n",
    "            sentence_right = sentence_right[1:]\n",
    "\n",
    "            # add the thing in context\n",
    "            full = sentence_left + \"<CITATION>\" + sentence_right\n",
    "\n",
    "            citation['contextPure'] = sentence_left\n",
    "            #print(full)\n",
    "\n",
    "            drep['citations'].append(citation)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        # now that we have all the information we need,\n",
    "        # we simply need to \"count\" this document in a few different ways\n",
    "        account_for(drep)\n",
    "\n",
    "\n",
    "    except ParseError as e:\n",
    "        print(\"parse error...\", e.args, doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fj 41\n",
      "fy 116\n",
      "c 461168\n",
      "fj.fy 1897\n",
      "c.fj 783972\n",
      "c.fy 908875\n",
      "c.t 1486352\n",
      "t 403677\n",
      "fy.t 517201\n",
      "fj.t 498395\n",
      "a.c 1329587\n",
      "a.fj.fy 68281\n",
      "a 37335\n",
      "c.c 1019768\n"
     ]
    }
   ],
   "source": [
    "for k,v in cnt_doc.items():\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving sociology-jstor.doc ___ fj\n",
      "Saving sociology-jstor.doc ___ fy\n",
      "Saving sociology-jstor.doc ___ c\n",
      "Saving sociology-jstor.doc ___ fj.fy\n",
      "Saving sociology-jstor.doc ___ c.fj\n",
      "Saving sociology-jstor.doc ___ c.fy\n",
      "Saving sociology-jstor.doc ___ c.t\n",
      "Saving sociology-jstor.doc ___ t\n",
      "Saving sociology-jstor.doc ___ fy.t\n",
      "Saving sociology-jstor.doc ___ fj.t\n",
      "Saving sociology-jstor.doc ___ a.c\n",
      "Saving sociology-jstor.doc ___ a.fj.fy\n",
      "Saving sociology-jstor.doc ___ a\n",
      "Saving sociology-jstor.doc ___ c.c\n"
     ]
    }
   ],
   "source": [
    "save_cnt(\"%s.doc\"%database_name, cnt_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving sociology-jstor.ind ___ fy\n",
      "Saving sociology-jstor.ind ___ c\n",
      "Saving sociology-jstor.ind ___ fj\n",
      "Saving sociology-jstor.ind ___ fj.fy\n",
      "Saving sociology-jstor.ind ___ c.fj\n",
      "Saving sociology-jstor.ind ___ c.fy\n",
      "Saving sociology-jstor.ind ___ c.t\n",
      "Saving sociology-jstor.ind ___ t\n",
      "Saving sociology-jstor.ind ___ fy.t\n",
      "Saving sociology-jstor.ind ___ fj.t\n",
      "Saving sociology-jstor.ind ___ a.c\n",
      "Saving sociology-jstor.ind ___ a.fj.fy\n",
      "Saving sociology-jstor.ind ___ a\n",
      "Saving sociology-jstor.ind ___ c.c\n"
     ]
    }
   ],
   "source": [
    "save_cnt(\"%s.ind\"%database_name, cnt_ind)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
