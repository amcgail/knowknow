{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(_dh[0].split(\"knowknow\")[0])\n",
    "from knowknow import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Counting coocurrences"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Cultural phenomena are rich in meaning and context. Moreover, the meaning and context are what we care about, so stripping that would be a disservice. \"Consider Geertz:\"\n",
       "> Not only is the semantic structure of the figure a good deal more complex than it appears on the surface, but an analysis of that structure forces one into tracing a multiplicity of referential connections between it and social reality, so that the final picture is one of a configuration of dissimilar meanings out of whose interworking both the expressive power and the rhetorical force of the final symbol derive. (Geertz [1955] 1973, Chapter 8 Ideology as a Cultural System, p. 213)\n",
       "\n",
       "The way people understanding their world shape their action, and understandings are heterogeneous in any community, woven into a complex web of interacting pieces and parts. Understandings are constantly evolving, shifting with every conversation or Breaking News. Any quantitative technique for studying meaning must be able to capture the relational structure of cultural objects, their temporal dynamics, or it cannot be meaning.\n",
       "\n",
       "These considerations motivate how I have designed the data structure and code for this project. My attention to \"cooccurrences\" in what follows is an application of Levi Martin and Lee's (2018) formal approach to meaning. They develop the symbolic formalism I use below, as well as showing several general analytic strategies for inductive, ground-up meaning-making from count data. This approach is quite general, useful for many applications.\n",
       "\n",
       "The process is rather simple, I count cooccurrences between various attributes. For each document, for each citation in that document, I increment a dozen counters, depending on attributes of the citation, paper, journal, or author. This counting process is done once, and can be used as a compressed form of the dataset for all further analyses. In the terminology of Levi Martin and Lee, I am constructing \"hypergraphs\", and I will use their notation in what follows. For example $[c*fy]$ indicates the dataset which maps from $(c, fy) \\to count$.\n",
       "$c$ is the name of the cited work. $fy$ is the publication year of the article which made the citation. $count$ is the number of citations which are at the intersection of these properties.\n",
       "\n",
       "+ $[c]$ the number of citations each document receives\n",
       "+ $[c*fj]$ the number of citations each document receives from each journal's articles\n",
       "+ $[c*fy]$ the number of citations each document receives from each year's articles\n",
       "+ $[fj]$ the number of citations from each journal\n",
       "+ $[fj*fy]$ the number of citations in each journal in each year\n",
       "+ $[t]$ cited term total counts\n",
       "+ $[fy*t]$ cited term time series\n",
       "+ term cooccurrence with citation and journal ($[c*t]$ and [fj*t]$)\n",
       "+ \"author\" counts, the number of citations by each author ($[a]$ $[a*c]$ $[a*j*y]$)\n",
       "+ [c*c]$, the cooccurrence network between citations\n",
       "+ the death of citations can be studied using the $[c*fy]$ hypergraph\n",
       "+ $[c*fj*t]$ could be used for analyzing differential associations of $c$ to $t$ across publication venues\n",
       "+ $[ta*ta]$, $[fa*fa]$, $[t*t]$ and $[c*c]$ open the door to network-scientific methods"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# References\n",
       "\n",
       "\n",
       "\n",
       "+ Martin, John Levi, and Monica Lee. 2018. “A Formal Approach to Meaning.” Poetics 68(February):10–17.\n",
       "+ Geertz, Clifford. 1973. The Interpretation of Cultures. New York: Basic Books, Inc."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "showdocs(\"counter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# How to retrieve the requisite data\n",
    "\n",
    "+ This notebook generates cooccurrence counts given Web of Science output.\n",
    "+ No Web of Science output is included in knowknow in compliance with Web of Science terms of use.\n",
    "+ **This notebook is only useful if you have your own Web of Science output**, and want to run analyses on that.\n",
    "\n",
    "Data can be downloaded from any Web of Science search results page:\n",
    "1. `Export -> Other File Formats`. \n",
    "2. In the dropdowns, specify `Record Content -> Full Record and Cited References` and `File Format -> Tab Delimited (Win)`\n",
    "3. Type the folder containing the saved records as `.txt` files into the variable `wos_base` below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "1. Edit the parameters to fit your needs\n",
    "2. Run all cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#wos_base = \"path/to/wos/data\"\n",
    "wos_base = \"G:/My Drive/projects/qualitative analysis of literature/pre 5-12-2020/009 get everything from WOS\"\n",
    "\n",
    "\n",
    "#journal_map = {\n",
    "#    \"journal of social forces\": \"social forces\",\\\n",
    "#    \"studies in symbolic interaction*\": \"studies in symbolic interaction\"    \n",
    "#}\n",
    "\n",
    "name_blacklist = [\n",
    "    \"*us\", 'Press', 'Knopf', '(January', 'Co', 'London', 'Bros', 'Books', 'Wilson','[anonymous]'\n",
    "]\n",
    "\n",
    "\n",
    "debug = False\n",
    "database_name = \"sociology-wos\"\n",
    "\n",
    "RUN_PREGROUP = True\n",
    "\n",
    "RUN_EVERYTHING = True\n",
    "\n",
    "\n",
    "use_included_citations_filter = False\n",
    "if use_included_citations_filter:\n",
    "    included_citations = load_variable(\"%s-all/included_citations\" % database_name)\n",
    "    \n",
    "    \n",
    "use_included_journals_filter = True\n",
    "journal_keep = [\"ETHNIC AND RACIAL STUDIES\", \"LAW & SOCIETY REVIEW\", \"DISCOURSE & SOCIETY\", \"SOCIOLOGICAL INQUIRY\", \"CONTRIBUTIONS TO INDIAN SOCIOLOGY\", \"SOCIETY & NATURAL RESOURCES\", \"RATIONALITY AND SOCIETY\", \"DEVIANT BEHAVIOR\", \"ACTA SOCIOLOGICA\", \"SOCIOLOGY-THE JOURNAL OF THE BRITISH SOCIOLOGICAL ASSOCIATION\", \"WORK EMPLOYMENT AND SOCIETY\", \"SOCIOLOGICAL METHODS & RESEARCH\", \"SOCIOLOGICAL PERSPECTIVES\", \"JOURNAL OF MARRIAGE AND FAMILY\", \"WORK AND OCCUPATIONS\", \"JOURNAL OF CONTEMPORARY ETHNOGRAPHY\", \"THEORY AND SOCIETY\", \"POLITICS & SOCIETY\", \"SOCIOLOGICAL SPECTRUM\", \"RACE & CLASS\", \"ANTHROZOOS\", \"LEISURE SCIENCES\", \"COMPARATIVE STUDIES IN SOCIETY AND HISTORY\", \"SOCIAL SCIENCE QUARTERLY\", \"MEDIA CULTURE & SOCIETY\", \"SOCIOLOGY OF HEALTH & ILLNESS\", \"SOCIOLOGIA RURALIS\", \"SOCIOLOGICAL REVIEW\", \"TEACHING SOCIOLOGY\", \"BRITISH JOURNAL OF SOCIOLOGY\", \"JOURNAL OF THE HISTORY OF SEXUALITY\", \"SOCIOLOGY OF EDUCATION\", \"SOCIAL NETWORKS\", \"ARMED FORCES & SOCIETY\", \"YOUTH & SOCIETY\", \"POPULATION AND DEVELOPMENT REVIEW\", \"SOCIETY\", \"JOURNAL OF HISTORICAL SOCIOLOGY\", \"HUMAN ECOLOGY\", \"INTERNATIONAL SOCIOLOGY\", \"SOCIAL FORCES\", \"EUROPEAN SOCIOLOGICAL REVIEW\", \"JOURNAL OF HEALTH AND SOCIAL BEHAVIOR\", \"SOCIOLOGICAL THEORY\", \"SOCIAL INDICATORS RESEARCH\", \"POETICS\", \"HUMAN STUDIES\", \"SOCIOLOGICAL FORUM\", \"AMERICAN SOCIOLOGICAL REVIEW\", \"SOCIOLOGY OF SPORT JOURNAL\", \"SOCIOLOGY OF RELIGION\", \"JOURNAL OF LAW AND SOCIETY\", \"GENDER & SOCIETY\", \"BRITISH JOURNAL OF SOCIOLOGY OF EDUCATION\", \"LANGUAGE IN SOCIETY\", \"AMERICAN JOURNAL OF ECONOMICS AND SOCIOLOGY\", \"ANNALS OF TOURISM RESEARCH\", \"SOCIAL PROBLEMS\", \"INTERNATIONAL JOURNAL OF INTERCULTURAL RELATIONS\", \"SOCIAL SCIENCE RESEARCH\", \"SYMBOLIC INTERACTION\", \"JOURNAL OF LEISURE RESEARCH\", \"ECONOMY AND SOCIETY\", \"SOCIAL COMPASS\", \"SOCIOLOGICAL QUARTERLY\", \"JOURNAL OF MATHEMATICAL SOCIOLOGY\", \"AMERICAN JOURNAL OF SOCIOLOGY\", \"REVIEW OF RELIGIOUS RESEARCH\", \"RURAL SOCIOLOGY\", \"JOURNAL FOR THE SCIENTIFIC STUDY OF RELIGION\", \"ARCHIVES EUROPEENNES DE SOCIOLOGIE\", \"CANADIAN JOURNAL OF SOCIOLOGY-CAHIERS CANADIENS DE SOCIOLOGIE\"]\n",
    "journal_keep = [x.lower() for x in journal_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groups successfully loaded.\n",
      "Loaded keys: dict_keys(['c'])\n",
      "Available keys: ['c', 'c.c', 'c.fj', 'c.fy', 'fj', 'fj.fy', 'fy', 'ty', 'ty.ty']\n",
      "Citations successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "groups=None\n",
    "\n",
    "if RUN_PREGROUP:    \n",
    "    # loading precomputed groupings of cited books and articles, if the grouping has been generated\n",
    "\n",
    "    try:\n",
    "        groups = load_variable(\"%s-all/groups\" % database_name)\n",
    "        ysumc = load_variable(\"%s-all/c.ysum\" % database_name)\n",
    "        print(\"Groups successfully loaded.\")\n",
    "        \n",
    "        # find the most popular representation\n",
    "        current_c = get_cnt(\"%s-all/doc\"%database_name, ['c'])\n",
    "        print(\"Citations successfully loaded.\")\n",
    "\n",
    "    except VariableNotFound:\n",
    "        print(\"Groups don't exist yet. It's important to incorporate a fuzzy match for cited references. \"\n",
    "              \"Run this script once, then run 'trend summaries/cysum.ipynb' to generate summaries and filter out small cited works. \"\n",
    "              \"Then run 'grouping article and book names.ipynb' to generate groupings. \"\n",
    "              \"Then run this notebook again to generate counts for grouped entries. \"\n",
    "              \"And finally, to make sure cysum is up to date, run 'trend summaries/cysum.ipynb' one more time. \"\n",
    "             )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First pass - just counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import DictReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcount = 0\n",
    "total_inserts = 0\n",
    "to_inserts = []\n",
    "basedir = Path(wos_base)\n",
    "\n",
    "# keeps track of all the years seen for grouped citations\n",
    "multi_year = defaultdict(lambda: defaultdict(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiating counters\n",
    "cnt_ind = defaultdict(lambda:defaultdict(int))\n",
    "track_doc = defaultdict(lambda:defaultdict(set))\n",
    "cnt_doc = defaultdict(lambda:defaultdict(int))\n",
    "\n",
    "def cnt(term, space, doc):\n",
    "    if \".\".join(sorted(space.split(\".\"))) != space:\n",
    "        raise Exception(space, \"should be sorted...\")\n",
    "    \n",
    "    # it's a set, yo\n",
    "    track_doc[space][term].add(doc)\n",
    "    # update cnt_doc\n",
    "    cnt_doc[space][term] = len(track_doc[space][term])\n",
    "    # update ind count\n",
    "    cnt_ind[space][term] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This cell ensures there are not overflow errors while importing large CSVs\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "maxInt = sys.maxsize\n",
    "\n",
    "while True:\n",
    "    # decrease the maxInt value by factor 10\n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fixcitedauth(a):\n",
    "    a = a.strip()\n",
    "    if not len(a):\n",
    "        return None\n",
    "\n",
    "    sp = a.lower().split()\n",
    "    if len(sp) < 2:\n",
    "        return None\n",
    "    if len(sp) >= 5:\n",
    "        return None\n",
    "    \n",
    "    l, f = a.lower().split()[:2] # take first two words\n",
    "    \n",
    "    if len(l) == 1: # sometimes last and first name are switched for some reason\n",
    "        l, f = f, l\n",
    "        \n",
    "    f = f[0] + \".\" # first initial\n",
    "    \n",
    "    a = \", \".join([l, f]) # first initial\n",
    "    a = a.title() # title format, so I don't have to worry later\n",
    "    \n",
    "    if debug:\n",
    "        print('cited author:', a)\n",
    "        \n",
    "    return a\n",
    "\n",
    "\n",
    "def fix_refs(refs):\n",
    "    for r in refs:\n",
    "        yspl = re.split(\"((?:18|19|20)[0-9]{2})\", r)\n",
    "\n",
    "        if len(yspl) < 2:\n",
    "            continue\n",
    "\n",
    "        auth, year = yspl[:2]\n",
    "        auth = auth.strip()\n",
    "        \n",
    "        if len(auth.split(\" \")) > 5:\n",
    "            continue\n",
    "        \n",
    "        year = int(year)\n",
    "\n",
    "        if auth == \"\":\n",
    "            continue\n",
    "\n",
    "        auth = fixcitedauth( auth )\n",
    "        if auth is None: # catching non-people, and not counting the citations\n",
    "            continue\n",
    "        \n",
    "        full_ref = r\n",
    "        \n",
    "        if 'DOI' not in full_ref and not ( # it's a book!\n",
    "            len(re.findall(r', [Vv][0-9]+', full_ref)) or\n",
    "            len(re.findall(r'[0-9]+, [Pp]?[0-9]+', full_ref))\n",
    "        ):\n",
    "            #full_ref = re.sub(r', [Pp][0-9]+', '', full_ref) # get rid of page numbers!\n",
    "            \n",
    "            full_ref = \"|\".join( # splits off the author and year, and takes until the next comma\n",
    "                [auth]+\n",
    "                [x.strip().lower() for x in full_ref.split(\",\")[2:3]]\n",
    "            )\n",
    "        else: # it's an article!\n",
    "            # just adds a fixed name and date to the front\n",
    "            full_ref = \"|\".join(\n",
    "                [auth, str(year)] +\n",
    "                [\",\".join( x.strip() for x in full_ref.lower().split(\",\")[2:] ).split(\",doi\")[0]]\n",
    "                )\n",
    "\n",
    "        if use_included_citations_filter:\n",
    "            if full_ref not in included_citations:\n",
    "                continue\n",
    "            \n",
    "        # implement grouping of references\n",
    "        if groups is not None:\n",
    "            if full_ref in groups:\n",
    "                # retrieves retrospectively-computed groups\n",
    "                full_ref = group_reps[\n",
    "                    groups[full_ref]\n",
    "                ]\n",
    "            elif full_ref in ysumc and '[no title captured]' not in full_ref:\n",
    "                # a small minority, the ones which are dropped in this process anyways\n",
    "                #print('not in grouping')\n",
    "                raise Exception(full_ref, \"not in grouping!\")\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "        if debug:\n",
    "            print('fix_refs_worked',auth,year,full_ref)\n",
    "        yield ( auth, year, full_ref )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-group cited references while counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_PREGROUP:\n",
    "    # loading precomputed groupings of cited books and articles, if the grouping has been generated\n",
    "    if groups is None:\n",
    "        print(\"Groups don't exist yet. It's important to incorporate a fuzzy match for cited references.\"\n",
    "              \"Run this script once, then run 'trend summaries/cysum.ipynb' to generate summaries and filter out small cited works.\"\n",
    "              \"Then run 'grouping article and book names.ipynb' to generate groupings.\"\n",
    "              \"Then run this notebook again to generate counts for grouped entries.\"\n",
    "              \"And finally, to make sure cysum is up to date, run 'trend summaries/cysum.ipynb' one more time.\"\n",
    "             )\n",
    "        raise Exception(\"Grouperror\")\n",
    "        \n",
    "\n",
    "    from collections import defaultdict\n",
    "\n",
    "    def get_reps(groups):\n",
    "        ret = defaultdict(set)\n",
    "        for k,v in groups.items():\n",
    "            ret[v].add(k)\n",
    "        ret = {\n",
    "            k: max(v, key=lambda x:current_c['c'][x])\n",
    "            for k,v in ret.items()\n",
    "        }\n",
    "        return ret\n",
    "\n",
    "    group_reps = get_reps(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 0/386: 74501-75000.txt\n",
      "Document: 10000\n",
      "Citations: 43016\n",
      "File 50/386: 3501-4000.txt\n",
      "Document: 20000\n",
      "Citations: 67163\n",
      "Document: 30000\n",
      "Citations: 75090\n",
      "File 100/386: 28501-29000.txt\n",
      "Document: 40000\n",
      "Citations: 83432\n",
      "File 150/386: 53501-54000.txt\n",
      "Document: 50000\n",
      "Citations: 86586\n",
      "Document: 60000\n",
      "Citations: 89273\n",
      "File 200/386: 4501-5000.txt\n",
      "Document: 70000\n",
      "Citations: 95138\n",
      "Document: 80000\n",
      "Citations: 100894\n",
      "File 250/386: 29501-30000.txt\n",
      "Document: 90000\n",
      "Citations: 104857\n",
      "Document: 100000\n",
      "Citations: 107454\n",
      "File 300/386: 54501-55000.txt\n",
      "Document: 110000\n",
      "Citations: 109413\n",
      "Document: 120000\n",
      "Citations: 110411\n",
      "Document: 130000\n",
      "Citations: 111036\n",
      "File 350/386: 79501-80000.txt\n",
      "Document: 140000\n",
      "Citations: 111506\n"
     ]
    }
   ],
   "source": [
    "# processes WoS txt output files one by one, counting relevant cooccurrences as it goes\n",
    "dcount=0\n",
    "for i, f in enumerate( list(basedir.glob(\"**/*.txt\")) ):\n",
    "\n",
    "    with f.open(encoding='utf8') as pfile:\n",
    "        r = DictReader(pfile, delimiter=\"\\t\")\n",
    "        rows = list(r)\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(\"File %s/%s: %s\" % (i, len(list(basedir.glob(\"**/*.txt\"))),f.name))\n",
    "\n",
    "    for i, r in enumerate(rows):\n",
    "        if r['DT'] != \"Article\":\n",
    "            continue\n",
    "            \n",
    "\n",
    "        #print(refs)\n",
    "\n",
    "        dcount += 1\n",
    "        if dcount % 10000 == 0:\n",
    "            print(\"Document: %s\" % dcount)\n",
    "            print(\"Citations: %s\" % len(cnt_doc['c']))\n",
    "            \n",
    "        if debug:\n",
    "            print(\"DOCUMENT %s\" % dcount)\n",
    "            if dcount > 10:\n",
    "                raise\n",
    "\n",
    "\n",
    "        refs = r[\"CR\"].strip().split(\";\")\n",
    "        refs = list( fix_refs(refs) )\n",
    "\n",
    "        if not len(refs):\n",
    "            continue\n",
    "\n",
    "\n",
    "        def fixcitingauth():\n",
    "            authors = r['AU'].split(\";\")\n",
    "            for x in authors:\n",
    "                x = x.strip().lower()\n",
    "                x = re.sub(\"[^a-zA-Z\\s,]+\", \"\", x) # only letters and spaces allowed\n",
    "                \n",
    "                xsp = x.split(\", \")\n",
    "                if len(xsp) < 2:\n",
    "                    return xsp[0]\n",
    "                elif len(xsp) > 2:\n",
    "                    raise Exception(\"author with too many commas\", x)\n",
    "                    \n",
    "                f, l = xsp[1], xsp[0]\n",
    "                f = f[0] # take only first initial of first name\n",
    "                \n",
    "                yield \"%s, %s\" % (l,f)\n",
    "            \n",
    "        citing_authors = list(fixcitingauth())\n",
    "        if not len(citing_authors):\n",
    "            continue\n",
    "        \n",
    "        if debug:\n",
    "            print(\"citing authors: \", citing_authors)\n",
    "\n",
    "        if False:\n",
    "            for i in range(10):\n",
    "                print(\"-\"*20)\n",
    "\n",
    "\n",
    "        uid = r['UT']\n",
    "\n",
    "\n",
    "        try:\n",
    "            int(r['PY'])\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        r['SO'] = r['SO'].lower() # REMEMBER THIS! lol everything is in lowercase... not case sensitive\n",
    "        \n",
    "        \n",
    "        if use_included_journals_filter and r['SO'].lower() not in journal_keep:\n",
    "            continue\n",
    "\n",
    "\n",
    "        for (auth,year,full_ref) in refs:\n",
    "                    \n",
    "            ref = (auth,year)\n",
    "           \n",
    "        \n",
    "            if ref[0] in name_blacklist:\n",
    "                continue\n",
    "            if \"*\" in ref[0]:\n",
    "                continue\n",
    "                \n",
    "            # BEGIN COUNTING!!!\n",
    "            \n",
    "            multi_year[full_ref][year] += 1\n",
    "                \n",
    "            cnt(r['SO'], 'fj', uid)\n",
    "            cnt(int(r['PY']), 'fy', uid)\n",
    "            cnt(ref[1], 'ty', uid)\n",
    "            cnt((full_ref, int(r['PY'])), 'c.fy', uid)\n",
    "            cnt((full_ref, r['SO']), 'c.fj', uid)\n",
    "\n",
    "            cnt((r['SO'],int(r['PY'])), 'fj.fy', uid)\n",
    "\n",
    "            cnt(full_ref, 'c', uid)\n",
    "            \n",
    "            if not RUN_EVERYTHING:\n",
    "                continue\n",
    "                \n",
    "            cnt((int(r['PY']), year), 'fy.ty', uid)\n",
    "            cnt((r['SO'], year), 'fj.ty', uid)\n",
    "                    \n",
    "            cnt(auth, 'ta', uid)\n",
    "            cnt((int(r['PY']),auth), 'fy.ta', uid)\n",
    "            cnt((r['SO'],auth), 'fj.ta', uid)\n",
    "            \n",
    "            # first author!\n",
    "            ffa = citing_authors[0]\n",
    "            cnt(ffa, 'ffa', uid)\n",
    "            cnt((ffa,int(r['PY'])), 'ffa.fy', uid)\n",
    "            cnt((ffa,r['SO']), 'ffa.fj', uid)\n",
    "            cnt((full_ref,ffa), 'c.ffa', uid)\n",
    "            #cnt((ffa,r['SO'], int(r['PY'])), 'ffa.fj.fy', uid)\n",
    "\n",
    "            for a in citing_authors:\n",
    "                cnt(a, 'fa', uid)\n",
    "                cnt((a,int(r['PY'])), 'fa.fy', uid)\n",
    "                cnt((a,r['SO']), 'fa.fj', uid)\n",
    "                #cnt((a,r['SO'], int(r['PY'])), 'fa.fj.fy', uid)\n",
    "\n",
    "                cnt((full_ref,a), 'c.fa', uid)\n",
    "            \n",
    "            cnt((full_ref, int(r['PY']), r['SO']), 'c.fy.j', uid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A SOCIOLOGICAL VIEW OF SOVEREIGNTY'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r['TI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Westermarck, E.|hist human marriage'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['llano, a']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citing_authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cocitation counter\n",
    "\n",
    "Because there are so many cited works, a full cocitation network would be prohibitively large to compute and store on disk. \n",
    "Furthermore, the full network is not very useful.\n",
    "The following creates a cocitation network among only the most common 1000 cited works.\n",
    "It counts the number of times `cnt['cc'][ (c1,c2) ]` that `c1` and `c2` are cited together in a work.\n",
    "The `ind` and `doc` counters are identical for this counter, `'cc'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# allowed references for cocitation analysis: 1000\n",
      "Examples: ['Bell, D.|coming post industri', 'Rosenberg, M.|society adolescent s', 'Lincoln, E.|black church african']\n",
      "File 0/386: G:\\My Drive\\projects\\qualitative analysis of literature\\pre 5-12-2020\\009 get everything from WOS\\74501-75000.txt\n",
      "10000 cocitations logged\n",
      "20000 cocitations logged\n",
      "30000 cocitations logged\n",
      "40000 cocitations logged\n",
      "50000 cocitations logged\n",
      "60000 cocitations logged\n",
      "70000 cocitations logged\n",
      "80000 cocitations logged\n",
      "90000 cocitations logged\n",
      "100000 cocitations logged\n",
      "110000 cocitations logged\n",
      "120000 cocitations logged\n",
      "File 50/386: G:\\My Drive\\projects\\qualitative analysis of literature\\pre 5-12-2020\\009 get everything from WOS\\3501-4000.txt\n",
      "130000 cocitations logged\n",
      "140000 cocitations logged\n",
      "150000 cocitations logged\n",
      "160000 cocitations logged\n",
      "170000 cocitations logged\n",
      "180000 cocitations logged\n",
      "190000 cocitations logged\n",
      "200000 cocitations logged\n",
      "210000 cocitations logged\n",
      "File 100/386: G:\\My Drive\\projects\\qualitative analysis of literature\\pre 5-12-2020\\009 get everything from WOS\\28501-29000.txt\n",
      "220000 cocitations logged\n",
      "230000 cocitations logged\n",
      "240000 cocitations logged\n",
      "250000 cocitations logged\n",
      "260000 cocitations logged\n",
      "270000 cocitations logged\n",
      "280000 cocitations logged\n",
      "290000 cocitations logged\n",
      "300000 cocitations logged\n",
      "310000 cocitations logged\n",
      "320000 cocitations logged\n",
      "330000 cocitations logged\n",
      "File 150/386: G:\\My Drive\\projects\\qualitative analysis of literature\\pre 5-12-2020\\009 get everything from WOS\\53501-54000.txt\n",
      "340000 cocitations logged\n",
      "350000 cocitations logged\n",
      "360000 cocitations logged\n",
      "370000 cocitations logged\n",
      "380000 cocitations logged\n",
      "390000 cocitations logged\n",
      "400000 cocitations logged\n",
      "410000 cocitations logged\n",
      "420000 cocitations logged\n",
      "430000 cocitations logged\n",
      "440000 cocitations logged\n",
      "450000 cocitations logged\n",
      "460000 cocitations logged\n",
      "File 200/386: G:\\My Drive\\projects\\qualitative analysis of literature\\pre 5-12-2020\\009 get everything from WOS\\2006 and back\\4501-5000.txt\n",
      "470000 cocitations logged\n",
      "480000 cocitations logged\n",
      "490000 cocitations logged\n",
      "500000 cocitations logged\n",
      "510000 cocitations logged\n",
      "520000 cocitations logged\n",
      "530000 cocitations logged\n",
      "540000 cocitations logged\n",
      "550000 cocitations logged\n",
      "560000 cocitations logged\n",
      "570000 cocitations logged\n",
      "580000 cocitations logged\n",
      "590000 cocitations logged\n",
      "600000 cocitations logged\n",
      "610000 cocitations logged\n",
      "620000 cocitations logged\n",
      "File 250/386: G:\\My Drive\\projects\\qualitative analysis of literature\\pre 5-12-2020\\009 get everything from WOS\\2006 and back\\29501-30000.txt\n",
      "630000 cocitations logged\n",
      "640000 cocitations logged\n",
      "650000 cocitations logged\n",
      "660000 cocitations logged\n",
      "670000 cocitations logged\n",
      "680000 cocitations logged\n",
      "690000 cocitations logged\n",
      "700000 cocitations logged\n",
      "710000 cocitations logged\n",
      "720000 cocitations logged\n",
      "730000 cocitations logged\n",
      "740000 cocitations logged\n",
      "750000 cocitations logged\n",
      "760000 cocitations logged\n",
      "File 300/386: G:\\My Drive\\projects\\qualitative analysis of literature\\pre 5-12-2020\\009 get everything from WOS\\2006 and back\\54501-55000.txt\n",
      "770000 cocitations logged\n",
      "780000 cocitations logged\n",
      "790000 cocitations logged\n",
      "800000 cocitations logged\n",
      "810000 cocitations logged\n",
      "820000 cocitations logged\n",
      "830000 cocitations logged\n",
      "840000 cocitations logged\n",
      "File 350/386: G:\\My Drive\\projects\\qualitative analysis of literature\\pre 5-12-2020\\009 get everything from WOS\\2006 and back\\79501-80000.txt\n",
      "850000 cocitations logged\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "if RUN_EVERYTHING:\n",
    "    allowed_refs = Counter(dict(cnt_ind['c'].items())).most_common(1000)\n",
    "    allowed_refs = set( x[0] for x in allowed_refs )\n",
    "\n",
    "    print(\"# allowed references for cocitation analysis: %s\" % len(allowed_refs))\n",
    "    print(\"Examples: %s\" % str(list(allowed_refs)[:3]))\n",
    "\n",
    "    # enumerating cocitation for works with at least 10 citations\n",
    "    dcount = 0\n",
    "    refcount = 0\n",
    "\n",
    "    for i, f in enumerate(list(basedir.glob(\"**/*.txt\"))):\n",
    "\n",
    "        with f.open(encoding='utf8') as pfile:\n",
    "            r = DictReader(pfile, delimiter=\"\\t\")\n",
    "            rows = list(r)\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(\"File %s/%s: %s\" % (i, len(list(basedir.glob(\"**/*.txt\"))), f))\n",
    "\n",
    "        for i, r in enumerate(rows):\n",
    "\n",
    "            if r['DT'] != \"Article\":\n",
    "                continue\n",
    "\n",
    "            refs = r[\"CR\"].strip().split(\";\")\n",
    "            refs = list(fix_refs(refs))\n",
    "\n",
    "            if not len(refs):\n",
    "                continue\n",
    "\n",
    "            uid = r['UT']\n",
    "\n",
    "            try:\n",
    "                int(r['PY'])\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            for (auth,year,full_ref) in refs:\n",
    "\n",
    "                if full_ref not in allowed_refs:\n",
    "                    continue\n",
    "\n",
    "                for (auth2,year2,full_ref2) in refs:\n",
    "\n",
    "                    if full_ref2 <= full_ref:\n",
    "                        continue\n",
    "\n",
    "                    if full_ref2 not in allowed_refs:\n",
    "                        continue\n",
    "\n",
    "                    cnt((full_ref,full_ref2), 'c.c', uid)\n",
    "                    cnt((year,year2), 'ty.ty', uid)\n",
    "\n",
    "                    refcount += 1\n",
    "                    if refcount % 10000 == 0:\n",
    "                        print(\"%s cocitations logged\" % refcount)\n",
    "\n",
    "    print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trim counters, excluding cited works with only a single reference\n",
    "\n",
    "Because of the huge number of cited works that only occur once, for efficiency it's best to simply log this statistic and eliminate them from the counter.\n",
    "\n",
    "\n",
    "*This might not be necessary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimCounters = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trimCounters:\n",
    "\n",
    "    onlyOne = len([x for x in cnt_doc['c'] if cnt_doc['c'][x] == 1])\n",
    "    total = len(cnt_doc['c'])\n",
    "    print(\"Of the %s cited works, %s (%0.2f%%) have only a single citation\" % (\n",
    "        total, onlyOne,\n",
    "        100 * onlyOne/total\n",
    "    ))\n",
    "\n",
    "    terms = list(cnt_doc['c'].keys())\n",
    "    counts = np.array([cnt_doc['c'][k] for k in terms])\n",
    "\n",
    "    cutoff = 2\n",
    "    to_remove = set([terms[int(i)] for i in np.argwhere(counts < cutoff)])\n",
    "    print(\"consolidating\", len(to_remove), list(to_remove)[:5])\n",
    "\n",
    "    cnt_doc.keys()\n",
    "\n",
    "    print(\"old size:\", len(cnt_doc['c']))\n",
    "    for tr in to_remove:\n",
    "        del cnt_doc['c'][tr]\n",
    "        del cnt_ind['c'][tr]\n",
    "    print(\"new size:\", len(cnt_doc['c']))\n",
    "\n",
    "    print(\"old size:\", len(cnt_doc['cy']))\n",
    "    tydels = [x for x in cnt_doc['cy'] if x[0] in to_remove]\n",
    "    for tydel in tydels:\n",
    "        del cnt_doc['cy'][tydel]\n",
    "        del cnt_ind['cy'][tydel]\n",
    "    print(\"new size:\", len(cnt_doc['cy']))\n",
    "\n",
    "    print(\"old size:\", len(cnt_doc['jc']))\n",
    "    jtdels = [x for x in cnt_doc['jc'] if x[1] in to_remove]\n",
    "    for jtdel in jtdels:\n",
    "        del cnt_doc['jc'][jtdel]\n",
    "        del cnt_ind['jc'][jtdel]\n",
    "    print(\"new size:\", len(cnt_doc['jc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111731 cited works\n"
     ]
    }
   ],
   "source": [
    "print(len(cnt_doc['c']), 'cited works')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export generated variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved sociology-wos.pubyears\n"
     ]
    }
   ],
   "source": [
    "# retrieve and use the MOST COMMON pub date for each\n",
    "pubyears = {\n",
    "    k:max(s.keys(), key=lambda x:multi_year[k][x]) for k,s in multi_year.items()\n",
    "    if len(s)\n",
    "}\n",
    "varname = \"%s.pubyears\"%database_name\n",
    "save_variable(varname, pubyears)\n",
    "print(\"saved %s\" % varname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving sociology-wos.doc ___ fj\n",
      "Saving sociology-wos.doc ___ fy\n",
      "Saving sociology-wos.doc ___ ty\n",
      "Saving sociology-wos.doc ___ c.fy\n",
      "Saving sociology-wos.doc ___ c.fj\n",
      "Saving sociology-wos.doc ___ fj.fy\n",
      "Saving sociology-wos.doc ___ c\n",
      "Saving sociology-wos.doc ___ fy.ty\n",
      "Saving sociology-wos.doc ___ fj.ty\n",
      "Saving sociology-wos.doc ___ ta\n",
      "Saving sociology-wos.doc ___ fy.ta\n",
      "Saving sociology-wos.doc ___ fj.ta\n",
      "Saving sociology-wos.doc ___ ffa\n",
      "Saving sociology-wos.doc ___ ffa.fy\n",
      "Saving sociology-wos.doc ___ ffa.fj\n",
      "Saving sociology-wos.doc ___ ffa.c\n",
      "Saving sociology-wos.doc ___ fa\n",
      "Saving sociology-wos.doc ___ fa.fy\n",
      "Saving sociology-wos.doc ___ fa.fj\n",
      "Saving sociology-wos.doc ___ fa.c\n",
      "Saving sociology-wos.doc ___ c.fy.j\n",
      "Saving sociology-wos.doc ___ c.c\n",
      "Saving sociology-wos.doc ___ ty.ty\n"
     ]
    }
   ],
   "source": [
    "save_cnt(\"%s/doc\"%database_name, cnt_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving sociology-wos.ind ___ fj\n",
      "Saving sociology-wos.ind ___ fy\n",
      "Saving sociology-wos.ind ___ ty\n",
      "Saving sociology-wos.ind ___ c.fy\n",
      "Saving sociology-wos.ind ___ c.fj\n",
      "Saving sociology-wos.ind ___ fj.fy\n",
      "Saving sociology-wos.ind ___ c\n",
      "Saving sociology-wos.ind ___ fy.ty\n",
      "Saving sociology-wos.ind ___ fj.ty\n",
      "Saving sociology-wos.ind ___ ta\n",
      "Saving sociology-wos.ind ___ fy.ta\n",
      "Saving sociology-wos.ind ___ fj.ta\n",
      "Saving sociology-wos.ind ___ ffa\n",
      "Saving sociology-wos.ind ___ ffa.fy\n",
      "Saving sociology-wos.ind ___ ffa.fj\n",
      "Saving sociology-wos.ind ___ ffa.c\n",
      "Saving sociology-wos.ind ___ fa\n",
      "Saving sociology-wos.ind ___ fa.fy\n",
      "Saving sociology-wos.ind ___ fa.fj\n",
      "Saving sociology-wos.ind ___ fa.c\n",
      "Saving sociology-wos.ind ___ c.fy.j\n",
      "Saving sociology-wos.ind ___ c.c\n",
      "Saving sociology-wos.ind ___ ty.ty\n"
     ]
    }
   ],
   "source": [
    "save_cnt(\"%s/ind\"%database_name, cnt_ind)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (qualitative analysis of literature)",
   "language": "python",
   "name": "pycharm-b9c7981b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
